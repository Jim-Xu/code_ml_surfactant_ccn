{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm = '/data/keeling/a/xx24/e/proj_ml/arm_data'\n",
    "dir = '/data/keeling/a/xx24/e/proj_ml/code_ml_surfactant_ccn'\n",
    "fp = '/data/keeling/a/xx24/e/proj_ml/code_ml_surfactant_ccn/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'MOS' # ASI BNF  COR  ENA  GUC  SGP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCN & CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 定义工作目录和文件列表\n",
    "data_dir = f'{arm}/{site}/'\n",
    "file_pattern = os.path.join(data_dir, f'{site.lower()}aosccn2colaavg*.nc')\n",
    "files = sorted(glob.glob(file_pattern))\n",
    "\n",
    "# 2) 指定我们关心的 supersaturation\n",
    "targets = [0.1, 0.2, 0.4, 0.8, 1.0]\n",
    "\n",
    "ccn_records = []\n",
    "cpc_records = []\n",
    "\n",
    "for fn in files:\n",
    "    ds = xr.open_dataset(fn)\n",
    "    try:\n",
    "        ss_calc = ds['supersaturation_calculated'].values.astype(float)\n",
    "        ss_rounded = np.round(ss_calc, 1)\n",
    "        n_ccn = ds['N_CCN'].values\n",
    "        n_cpc = ds['aerosol_number_concentration'].values\n",
    "        times = pd.to_datetime(ds['time'].values)\n",
    "        date = times[0].strftime('%Y-%m-%d')\n",
    "\n",
    "        # 1. CCN 按 supersaturation 分组\n",
    "        for tgt in targets:\n",
    "            idx = np.where(ss_rounded == tgt)[0]\n",
    "            if idx.size == 0:\n",
    "                continue\n",
    "            mean_ccn = np.nanmean(n_ccn[idx])\n",
    "            ccn_records.append({\n",
    "                'date': date,\n",
    "                'supersaturation': tgt,\n",
    "                'N_CCN': mean_ccn\n",
    "            })\n",
    "        \n",
    "        # 2. CPC 直接每天一个均值\n",
    "        mean_cpc = np.nanmean(n_cpc)\n",
    "        cpc_records.append({\n",
    "            'date': date,\n",
    "            'N_CPC': mean_cpc\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error in file {fn}: {e}\")\n",
    "    finally:\n",
    "        ds.close()\n",
    "\n",
    "# CCN pivot 表\n",
    "df_ccn = pd.DataFrame(ccn_records)\n",
    "df_ccn = df_ccn.groupby(['date', 'supersaturation'], as_index=False).mean()\n",
    "df_ccn_pivot = df_ccn.pivot(index='date', columns='supersaturation', values='N_CCN')\n",
    "\n",
    "# CPC 表\n",
    "df_cpc = pd.DataFrame(cpc_records)\n",
    "df_cpc = df_cpc.groupby('date', as_index=False).mean().set_index('date')\n",
    "\n",
    "# 保存\n",
    "df_ccn_pivot.to_csv(f'{fp}{site}_daily_N_CCN_by_supersaturation.csv')\n",
    "df_cpc.to_csv(f'{fp}{site}_daily_N_CPC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aerosol species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 定义工作目录及文件匹配\n",
    "data_dir = f'{arm}/{site}/'\n",
    "file_pattern = os.path.join(data_dir, f'{site.lower()}*acsm*.nc')\n",
    "files = sorted(glob.glob(file_pattern))\n",
    "\n",
    "# 2) 关注的 species 列表\n",
    "base_species = [\n",
    "    'total_organics',\n",
    "    'sulfate',      \n",
    "    'ammonium',  \n",
    "    'nitrate',     \n",
    "    'chloride'    \n",
    "]\n",
    "\n",
    "is_cdce = 'acsmcdce' in files[0]\n",
    "# 动态决定变量名\n",
    "species = [f\"{s}_CDCE\" if is_cdce else s for s in base_species]\n",
    "\n",
    "\n",
    "records = []\n",
    "for fn in files:\n",
    "    ds = xr.open_dataset(fn)\n",
    "    \n",
    "    # 3) 提取日期：优先 time 维度第一个值，否则从文件名中解析 yyyymmdd \n",
    "    date = pd.to_datetime(ds['time'].values[0]).strftime('%Y-%m-%d')\n",
    "\n",
    "    # 4) 对每个 species 过滤负值并计算平均\n",
    "    rec = {'date': date}\n",
    "    for sp in species:\n",
    "        if sp not in ds:\n",
    "            rec[sp] = np.nan\n",
    "            continue\n",
    "        data = ds[sp].values\n",
    "        # 如果多维，以 time 维度 (假设为第一维) 为主 flatten\n",
    "        flat = np.ravel(data)\n",
    "        # 只保留非负值\n",
    "        pos = flat[flat >= 0]\n",
    "        rec[sp] = np.nan if pos.size == 0 else np.nanmean(pos)\n",
    "    records.append(rec)\n",
    "    ds.close()\n",
    "\n",
    "# 5) 构造 DataFrame，并以日期排序\n",
    "df_acsm = pd.DataFrame(records)\n",
    "df_acsm['date'] = pd.to_datetime(df_acsm['date'])\n",
    "df_acsm = df_acsm.set_index('date').sort_index()\n",
    "\n",
    "# 6) 保存或查看\n",
    "df_acsm.to_csv(f'{fp}{site}_daily_ACSM_species.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meteorology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n",
      "/tmp/ipykernel_124351/141437787.py:23: RuntimeWarning: Mean of empty slice\n",
      "  rec[var] = np.nanmean(flat)\n"
     ]
    }
   ],
   "source": [
    "# 1) 定义 MET 数据目录与文件模式\n",
    "data_dir = f'{arm}/{site}/'\n",
    "file_pattern = os.path.join(data_dir, f'{site.lower()}*met*')\n",
    "files = sorted(glob.glob(file_pattern))\n",
    "\n",
    "records = []\n",
    "for fn in files:\n",
    "    # 2) 打开数据集\n",
    "    ds = xr.open_dataset(fn)\n",
    "    \n",
    "    # 3) 提取日期标签\n",
    "    date = pd.to_datetime(ds['time'].values[0]).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # 4) 读取 temp_mean 与 rh_mean，并计算日平均（跳过 NaN）\n",
    "    rec = {'date': date}\n",
    "    for var in ('temp_mean', 'rh_mean'):    \n",
    "        data = ds[var].values\n",
    "        flat = np.ravel(data)\n",
    "        if var == 'rh_mean':\n",
    "            flat = flat / 100\n",
    "        if var == 'temp_mean':\n",
    "            flat = flat + 273.15\n",
    "        rec[var] = np.nanmean(flat)\n",
    "    \n",
    "    ds.close()\n",
    "    records.append(rec)\n",
    "\n",
    "# 5) 构造 DataFrame，并按日期排序\n",
    "df_met = pd.DataFrame(records)\n",
    "df_met['date'] = pd.to_datetime(df_met['date'])\n",
    "df_met = df_met.set_index('date').sort_index()\n",
    "\n",
    "# 6) 保存结果或查看\n",
    "df_met.to_csv(f'{fp}{site}_daily_MET.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bulk diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  D_bulk (μm)\n",
      "0  2019-10-11     0.066771\n",
      "1  2019-10-12     0.066085\n",
      "2  2019-10-13     0.042541\n",
      "3  2019-10-14     0.067755\n",
      "4  2019-10-15     0.048528\n"
     ]
    }
   ],
   "source": [
    "# 1. 定义目录和文件列表\n",
    "data_dir = f'{arm}/{site}/'\n",
    "# 支持所有含smps或aps的文件，不区分大小写\n",
    "smps_files = sorted(glob.glob(os.path.join(data_dir, '*[sS][mM][pP][sS]*')))\n",
    "aps_files = sorted(glob.glob(os.path.join(data_dir, '*[aA][pP][sS]*')))\n",
    "\n",
    "def get_var(ds, varnames):\n",
    "    \"\"\"依次尝试取变量名，返回第一个存在的变量\"\"\"\n",
    "    for name in varnames:\n",
    "        if name in ds.variables:\n",
    "            return ds[name]\n",
    "    raise KeyError(f\"变量 {varnames} 在该文件中都找不到\")\n",
    "\n",
    "def merge_all_size_distributions(files, is_aps=False):\n",
    "    \"\"\"按bin结构分组，每组单独拼接返回，最终输出所有天的数据\"\"\"\n",
    "    from collections import defaultdict\n",
    "    bin_groups = defaultdict(list)\n",
    "    for fn in files:\n",
    "        ds = xr.open_dataset(fn)\n",
    "        dNdlogDp = get_var(ds, ['merged_dN_dlogDp', 'dN_dlogDp'])\n",
    "        if is_aps:\n",
    "            Dp_nm = get_var(ds, ['diameter_aerodynamic'])\n",
    "        else:\n",
    "            Dp_nm = get_var(ds, ['merged_diameter_mobility', 'diameter_mobility'])\n",
    "        times = pd.to_datetime(ds['time'].values)\n",
    "        # 用bin长度作为分组依据\n",
    "        bin_key = len(Dp_nm)\n",
    "        bin_groups[bin_key].append((dNdlogDp.values, Dp_nm.values, times))\n",
    "        ds.close()\n",
    "    all_days = []\n",
    "    # 每组bin独立拼接\n",
    "    for group in bin_groups.values():\n",
    "        arr_list, Dp_list, times_list = zip(*group)\n",
    "        arr_cat = np.concatenate(arr_list, axis=0)\n",
    "        Dp_cat  = Dp_list[0]\n",
    "        times_cat = np.concatenate(times_list)\n",
    "        df_daily = calc_bulk_diameter(times_cat, arr_cat, Dp_cat)\n",
    "        all_days.append(df_daily)\n",
    "    # 合并所有天（日均bulk diameter, 若有重复取均值）\n",
    "    df_all = pd.concat(all_days).groupby('date', as_index=False).mean()\n",
    "    return df_all\n",
    "\n",
    "def merge_smps_aps(smps_files, aps_files):\n",
    "    time_smps, arr_smps, Dp_smps = merge_all_size_distributions(smps_files, is_aps=False)\n",
    "    time_aps,  arr_aps,  Dp_aps  = merge_all_size_distributions(aps_files, is_aps=True)\n",
    "    # 时间对齐\n",
    "    common_times = np.intersect1d(time_smps, time_aps)\n",
    "    if len(common_times) == 0:\n",
    "        raise ValueError(\"SMPS和APS没有共同时间，无法merge\")\n",
    "    idx_smps = np.where(np.isin(time_smps, common_times))[0]\n",
    "    idx_aps  = np.where(np.isin(time_aps,  common_times))[0]\n",
    "    arr_smps_sel = arr_smps[idx_smps]\n",
    "    arr_aps_sel  = arr_aps[idx_aps]\n",
    "    arr_merged = np.concatenate([arr_smps_sel, arr_aps_sel], axis=1)\n",
    "    Dp_merged  = np.concatenate([Dp_smps, Dp_aps])\n",
    "    return common_times, arr_merged, Dp_merged\n",
    "\n",
    "def calc_bulk_diameter(times, arr, Dp_nm):\n",
    "    Dp_um = Dp_nm / 1e3\n",
    "    logDp = np.log10(Dp_um)\n",
    "    dlog = np.empty_like(logDp)\n",
    "    dlog[1:] = logDp[1:] - logDp[:-1]\n",
    "    dlog[0] = dlog[1]\n",
    "    Ni = arr * dlog\n",
    "    Ntot = np.nansum(Ni, axis=1)\n",
    "    weighted_sum = np.nansum(Ni * Dp_um, axis=1)\n",
    "    valid = (Ntot > 0) & (~np.isnan(Ntot))\n",
    "    D_bulk_t = np.full_like(weighted_sum, np.nan)\n",
    "    D_bulk_t[valid] = weighted_sum[valid] / Ntot[valid]\n",
    "    df = pd.DataFrame({'time': pd.to_datetime(times), 'D_bulk (μm)': D_bulk_t})\n",
    "    df['date'] = df['time'].dt.strftime('%Y-%m-%d')\n",
    "    df_daily = df.groupby('date')['D_bulk (μm)'].mean().reset_index()\n",
    "    return df_daily\n",
    "\n",
    "records = []\n",
    "\n",
    "if len(aps_files) > 0:\n",
    "    times, arr, Dp_merged = merge_smps_aps(smps_files, aps_files)\n",
    "    df_daily = calc_bulk_diameter(times, arr, Dp_merged)\n",
    "else:\n",
    "    df_daily = merge_all_size_distributions(smps_files, is_aps=False)\n",
    "\n",
    "\n",
    "\n",
    "df_daily = df_daily.sort_values('date').reset_index(drop=True)\n",
    "df_daily.to_csv(f'{fp}{site}_daily_bulk_diameter.csv', index=False)\n",
    "print(df_daily.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_daily_data(set_point):\n",
    "    sp_str = str(set_point)\n",
    "\n",
    "    # 1) 读取各数据文件，各自只丢掉自己关心列的 NaN\n",
    "    #    —— bulk、acsm、met、cpc 只有一列到几列，直接 dropna(how='any') 是可以的\n",
    "    # acsm = (\n",
    "    #     pd.read_csv(\n",
    "    #         f'{dir}/data/{site}_daily_ACSM_species.csv',\n",
    "    #         parse_dates=['date'], index_col='date'\n",
    "    #     )\n",
    "    #     .dropna(how='any')\n",
    "    # )\n",
    "    bulk = (\n",
    "        pd.read_csv(\n",
    "            f'{dir}/data/{site}_daily_bulk_diameter.csv',\n",
    "            parse_dates=['date'], index_col='date'\n",
    "        )\n",
    "        .dropna(how='any')\n",
    "    )\n",
    "    met = (\n",
    "        pd.read_csv(\n",
    "            f'{dir}/data/{site}_daily_MET.csv',\n",
    "            parse_dates=['date'], index_col='date'\n",
    "        )\n",
    "        .dropna(how='any')\n",
    "    )\n",
    "    cpc = (\n",
    "        pd.read_csv(\n",
    "            f'{dir}/data/{site}_daily_N_CPC.csv',\n",
    "            parse_dates=['date'], index_col='date'\n",
    "        )\n",
    "        .dropna(how='any')\n",
    "    )\n",
    "\n",
    "    # 对 ccn 只选取我们关心的那一列，再 dropna\n",
    "    raw_ccn = pd.read_csv(\n",
    "        f'{dir}/data/{site}_daily_N_CCN_by_supersaturation.csv',\n",
    "        parse_dates=['date'], index_col='date'\n",
    "    )\n",
    "    # 只对 sp_str 那一列做 dropna\n",
    "    df_ccn_sp = (\n",
    "        raw_ccn[[sp_str]]\n",
    "        .rename(columns={sp_str: 'N_CCN'})\n",
    "        .dropna(how='any')\n",
    "    )\n",
    "\n",
    "    # 2) 选择我们要的列名并重命名\n",
    "    bulk_col = 'D_bulk (μm)'\n",
    "    df_bulk_sp = bulk[[bulk_col]].rename(columns={bulk_col: 'bulk_diameter'})\n",
    "\n",
    "    df_cpc_sp = cpc[['N_CPC']]  # 已经只有一列 “N_CPC”\n",
    "\n",
    "    # 3) 依次 inner join —— 只保留所有表都有值的日期\n",
    "    df = (\n",
    "        met[['temp', 'rh']]\n",
    "        .join(df_bulk_sp,    how='inner')\n",
    "        .join(df_cpc_sp,     how='inner')\n",
    "        # .join(acsm,          how='inner')\n",
    "        .join(df_ccn_sp,     how='inner')\n",
    "    )\n",
    "\n",
    "    # 4) 最后再统一 drop 一次（以防 join 之后还有任意 NaN）\n",
    "    df = df.dropna(how='any')\n",
    "\n",
    "    # 5) 写出结果\n",
    "    df = df.reset_index()\n",
    "    outfile = f'{fp}{site}_daily_merged_setpoint_{sp_str}.csv'\n",
    "    df.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.1, 0.2, 0.4, 0.8, 1.0]\n",
    "if __name__ == '__main__':\n",
    "    merge_daily_data(0.1)\n",
    "    merge_daily_data(0.2)\n",
    "    merge_daily_data(0.4)\n",
    "    merge_daily_data(0.8)\n",
    "    merge_daily_data(1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
